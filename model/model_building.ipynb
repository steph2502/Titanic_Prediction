# Titanic Survival Prediction - Model Development

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import joblib
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Load the Titanic dataset
print("Loading Titanic dataset...")
url = "https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv"
df = pd.read_csv(url)

print(f"Dataset shape: {df.shape}")
print("\nFirst few rows:")
print(df.head())
print("\nDataset info:")
print(df.info())

# 2. Data Preprocessing

# Select 5 features: Pclass, Sex, Age, Fare, Embarked
selected_features = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']
target = 'Survived'

# Create a copy with selected features
data = df[selected_features + [target]].copy()

print("\n" + "="*50)
print("DATA PREPROCESSING")
print("="*50)

# a. Handling missing values
print("\nMissing values before handling:")
print(data.isnull().sum())

# Fill missing Age with median
data['Age'].fillna(data['Age'].median(), inplace=True)

# Fill missing Embarked with mode
data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)

# Fill missing Fare with median
data['Fare'].fillna(data['Fare'].median(), inplace=True)

print("\nMissing values after handling:")
print(data.isnull().sum())

# c. Encoding categorical variables
print("\n" + "="*50)
print("ENCODING CATEGORICAL VARIABLES")
print("="*50)

# Encode Sex: male=1, female=0
le_sex = LabelEncoder()
data['Sex'] = le_sex.fit_transform(data['Sex'])
print(f"Sex encoding: {dict(zip(le_sex.classes_, le_sex.transform(le_sex.classes_)))}")

# Encode Embarked: C=0, Q=1, S=2
le_embarked = LabelEncoder()
data['Embarked'] = le_embarked.fit_transform(data['Embarked'])
print(f"Embarked encoding: {dict(zip(le_embarked.classes_, le_embarked.transform(le_embarked.classes_)))}")

# b. Feature selection - Already done (5 features selected)
X = data[selected_features]
y = data[target]

print("\nFeatures selected:", selected_features)
print("Feature matrix shape:", X.shape)
print("Target variable shape:", y.shape)

# d. Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=selected_features)

print("\nFeature scaling completed")
print(X_scaled.head())

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nTraining set size: {X_train.shape[0]}")
print(f"Testing set size: {X_test.shape[0]}")

# 3. Implement Random Forest Classifier
print("\n" + "="*50)
print("MODEL TRAINING - RANDOM FOREST CLASSIFIER")
print("="*50)

model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    random_state=42,
    min_samples_split=5,
    min_samples_leaf=2
)

# 4. Train the model
print("\nTraining the model...")
model.fit(X_train, y_train)
print("Model training completed!")

# 5. Evaluate the model
print("\n" + "="*50)
print("MODEL EVALUATION")
print("="*50)

# Predictions
y_pred_train = model.predict(X_train)
y_pred_test = model.predict(X_test)

# Training accuracy
train_accuracy = accuracy_score(y_train, y_pred_train)
print(f"\nTraining Accuracy: {train_accuracy:.4f}")

# Testing accuracy
test_accuracy = accuracy_score(y_test, y_pred_test)
print(f"Testing Accuracy: {test_accuracy:.4f}")

# Classification Report
print("\nClassification Report (Test Set):")
print(classification_report(y_test, y_pred_test, target_names=['Did Not Survive', 'Survived']))

# Confusion Matrix
print("\nConfusion Matrix:")
cm = confusion_matrix(y_test, y_pred_test)
print(cm)

# Feature Importance
print("\nFeature Importance:")
feature_importance = pd.DataFrame({
    'Feature': selected_features,
    'Importance': model.feature_importances_
}).sort_values('Importance', ascending=False)
print(feature_importance)

# 6. Save the trained model and preprocessing objects
print("\n" + "="*50)
print("SAVING MODEL AND PREPROCESSORS")
print("="*50)

# Save model
joblib.dump(model, 'titanic_survival_model.pkl')
print("✓ Model saved as 'titanic_survival_model.pkl'")

# Save scaler
joblib.dump(scaler, 'scaler.pkl')
print("✓ Scaler saved as 'scaler.pkl'")

# Save label encoders
joblib.dump(le_sex, 'le_sex.pkl')
joblib.dump(le_embarked, 'le_embarked.pkl')
print("✓ Label encoders saved")

# Save feature names for reference
joblib.dump(selected_features, 'feature_names.pkl')
print("✓ Feature names saved")

# 7. Demonstrate reloading and prediction
print("\n" + "="*50)
print("DEMONSTRATING MODEL RELOADING")
print("="*50)

# Reload the model
loaded_model = joblib.load('titanic_survival_model.pkl')
loaded_scaler = joblib.load('scaler.pkl')
print("✓ Model and scaler reloaded successfully")

# Test prediction with sample data
sample_passenger = pd.DataFrame({
    'Pclass': [3],
    'Sex': [1],  # male
    'Age': [22],
    'Fare': [7.25],
    'Embarked': [2]  # S
})

print("\nSample Passenger Data:")
print(sample_passenger)

# Scale the input
sample_scaled = loaded_scaler.transform(sample_passenger)

# Make prediction
prediction = loaded_model.predict(sample_scaled)
probability = loaded_model.predict_proba(sample_scaled)

print(f"\nPrediction: {'Survived' if prediction[0] == 1 else 'Did Not Survive'}")
print(f"Survival Probability: {probability[0][1]:.2%}")

print("\n" + "="*50)
print("MODEL DEVELOPMENT COMPLETED SUCCESSFULLY!")
print("="*50)